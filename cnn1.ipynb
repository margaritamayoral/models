{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/margaritamayoral/models/blob/master/cnn1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "D0xXvAuTF69g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importing tensorflow\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XtNTDnUEIAb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "4cb61ad8-114e-4b58-e60c-88318a911cbe"
      },
      "cell_type": "code",
      "source": [
        "#Importing MNIST dataset\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) \n",
        "#one_hot vectors is 0 in most dimensions except for a single which denotes a value\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-ef91251a98f8>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K6kKir0GKWRt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating placeholders\n",
        "x=tf.placeholder(tf.float32, [None, 784])\n",
        "#placeholder is not a variable. It is a value created for tensor flow to create computation.\n",
        "#MNIST data is in 28*28 pixel shape which needs to be flattened into 784 dimension vector (28*28=784)\n",
        "#Thus a 2-D tensor is created with a shape of [None, 784] (None means deimension can be of varied lenght)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SouIsmtqL1Kh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#creating weights and biases\n",
        "w=tf.Variable(tf.zeros([784,10]))\n",
        "b=tf.Variable(tf.zeros([10]))\n",
        "\n",
        "#Variables here is a value that lives in TensorFlow's computation graph. This can be changed and used by the computation.\n",
        "#initially w, b are zeros of (784,10) and (10) in dimension because it needs to learn the values so it does not matter if the initial values are zeros\n",
        "#our model is y=softmax((x*w)+b). Thus w is of dimension (784*10). Matrix multiplication of x and w is going to result\n",
        "#in a vector of shape (10). This can be added to b."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j5MFw8ZFyd1V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#our model\n",
        "y=tf.nn.softmax(tf.matmul(x,w) + b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "07ISbDOCkDq-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The cross-entropy function is a cost or loss function. It compares the true values to the predicted values. The goal is to minimize the loss"
      ]
    },
    {
      "metadata": {
        "id": "XgLCUeoOnu5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are many situations where cross-entropy needs to be measured but the distribution of p is unknown. An example is language modeling, where a model is created based on a training set  T, and then its cross-entropy is measured on a test set to assess how accurate the model is in predicting the test data. In this example,  p is the true distribution of words in any corpus, and  q is the distribution of words as predicted by the model. Since the true distribution is unknown, cross-entropy cannot be directly calculated. In these cases, an estimate of cross-entropy is calculated using the following formula:\n",
        "\n",
        "\\begin{aligned} H(T,q)=-\\sum _{i=1}^{N}{\\frac {1}{N}}\\log _{2}q(x_{i})} H(T,q)=-\\sum _{{i=1}}^{N}{\\frac  {1}{N}}\\log _{2}q(x_{i}) \\end{aligned}\n",
        "\n",
        "where  N is the size of the test set, and  q(x) is the probability of event x estimated from the training set. The sum is calculated over  N. This is a Monte Carlo estimate of the true cross entropy, where the training set is treated as samples from p(x)."
      ]
    },
    {
      "metadata": {
        "id": "E3_CnEzXn4dx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Cross entropy can be used to define the loss function in machine learning and optimization. The true probability {\\displaystyle p_{i}} p_{i} is the true label, and the given distribution {\\displaystyle q_{i}} q_{i} is the predicted value of the current model.\n",
        "\n",
        "More specifically, let us consider logistic regression, which (in its most basic form) deals with classifying a given set of data points into two possible classes generically labelled {\\displaystyle 0} {\\displaystyle 0} and {\\displaystyle 1} 1. The logistic regression model thus predicts an output {\\displaystyle y\\in \\{0,1\\}} y\\in \\{0,1\\}, given an input vector {\\displaystyle \\mathbf {x} } \\mathbf {x} . The probability is modeled using the logistic function {\\displaystyle g(z)=1/(1+e^{-z})} g(z)=1/(1+e^{{-z}}). Namely, the probability of finding the output {\\displaystyle y=1} y=1 is given by\n",
        "\n",
        "{\\displaystyle q_{y=1}\\ =\\ {\\hat {y}}\\ \\equiv \\ g(\\mathbf {w} \\cdot \\mathbf {x} )\\ =1/(1+e^{-\\mathbf {w} \\cdot \\mathbf {x} }),} {\\displaystyle q_{y=1}\\ =\\ {\\hat {y}}\\ \\equiv \\ g(\\mathbf {w} \\cdot \\mathbf {x} )\\ =1/(1+e^{-\\mathbf {w} \\cdot \\mathbf {x} }),}\n",
        "where the vector of weights {\\displaystyle \\mathbf {w} } \\mathbf {w}  is optimized through some appropriate algorithm such as gradient descent. Similarly, the complementary probability of finding the output {\\displaystyle y=0} y=0 is simply given by\n",
        "\n",
        "{\\displaystyle q_{y=0}\\ =\\ 1-{\\hat {y}}} q_{{y=0}}\\ =\\ 1-{\\hat  {y}}\n",
        "The true (observed) probabilities can be expressed similarly as {\\displaystyle p_{y=1}=y} p_{{y=1}}=y and {\\displaystyle p_{y=0}=1-y} p_{{y=0}}=1-y.\n",
        "\n",
        "Having set up our notation, {\\displaystyle p\\in \\{y,1-y\\}} p\\in \\{y,1-y\\} and {\\displaystyle q\\in \\{{\\hat {y}},1-{\\hat {y}}\\}} q\\in \\{{\\hat  {y}},1-{\\hat  {y}}\\}, we can use cross entropy to get a measure of dissimilarity between {\\displaystyle p} p and {\\displaystyle q} q:\n",
        "\n",
        "\\begin{aligned}H(p,q)\\ &=\\ -\\sum _{i}p_{i}\\log q_{i}\\ =\\ -y\\log {\\hat {y}}-(1-y)\\log(1-{\\hat {y}})} H(p,q)\\ =\\ -\\sum _{i}p_{i}\\log q_{i}\\ =\\ -y\\log {\\hat  {y}}-(1-y)\\log(1-{\\hat  {y}})\\,,\\end{aligned}}}\n",
        "\n",
        "The typical cost function that one uses in logistic regression is computed by taking the average of all cross-entropies in the sample. For example, suppose we have {\\displaystyle N} N samples with each sample labeled by {\\displaystyle n=1,\\dots ,N} n=1,\\dots ,N. The loss function is then given by:\n",
        "\n",
        "\\begin{aligned}J(\\mathbf {w} )\\ &=\\ {\\frac {1}{N}}\\sum _{n=1}^{N}H(p_{n},q_{n})\\ =\\ -{\\frac {1}{N}}\\sum _{n=1}^{N}\\ {\\bigg [}y_{n}\\log {\\hat {y}}_{n}+(1-y_{n})\\log(1-{\\hat {y}}_{n}){\\bigg ]}\\,,\\end{aligned}}} \\begin{aligned}J(\\mathbf {w} )\\ &=\\ {\\frac {1}{N}}\\sum _{n=1}^{N}H(p_{n},q_{n})\\ =\\ -{\\frac {1}{N}}\\sum _{n=1}^{N}\\ {\\bigg [}y_{n}\\log {\\hat {y}}_{n}+(1-y_{n})\\log(1-{\\hat {y}}_{n}){\\bigg ]}\\,,\\end{aligned}}}\n",
        "where {\\displaystyle {\\hat {y}}_{n}\\equiv g(\\mathbf {w} \\cdot \\mathbf {x} _{n})=1/(1+e^{-\\mathbf {w} \\cdot \\mathbf {x} _{n}})} {\\displaystyle {\\hat {y}}_{n}\\equiv g(\\mathbf {w} \\cdot \\mathbf {x} _{n})=1/(1+e^{-\\mathbf {w} \\cdot \\mathbf {x} _{n}})}, with {\\displaystyle g(z)} g(z) the logistic function as before.\n",
        "\n",
        "The logistic loss is sometimes called cross-entropy loss. It is also known as log loss (In this case, the binary label is often denoted by {-1,+1}).[2]\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "SfKV68Qei_Sm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#training our model\n",
        "y_=tf.placeholder(tf.float32, [None, 10])\n",
        "#cross entropy function \n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y), reduction_indices=[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R1c70VVZHcay",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gradient descent optimizer\n",
        "When a model graph is formed using the cross-entropy function we would want to find a point where the loss is a minimum. This is done using the Gradient Descent optimizer. It moves towards the part of the graph where the value of the graph is lesser. The steps or the learning rate can be set manually. If a very small learning rate like 0.001 is set it would take forever for the system to reach to the point where loss is minimum but it would be more accurate. If the learning rate is set high then the system may produce quick but false results. "
      ]
    },
    {
      "metadata": {
        "id": "niTNMnuWkCHU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_step= tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
        "#gradient optimizer tries to move to that direction in the computation graph where the cost or loss is reduced\n",
        "#learning rate/step lenght is 0.5. It will descend through croess entropy\n",
        "#init = tf.initialize_all_variables()\n",
        "init = tf.global_variables_initializer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ml5ccUPQ1Q89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Training Model\n",
        "#interactive session\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "#tensorflow uses a C++ backend. The connection with the backend is a session. Normally, a computation graph is created and the session launched.\n",
        "\n",
        "for i in range(100):\n",
        "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
        "  sess.run(train_step, feed_dict={x : batch_xs, y_:batch_ys})\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T6KPV5X_bl7O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluating and testing the model. \n",
        "Here the model is evaluated and tested with accuracy of 0.8916 . tf.argmax gives the  highest value in one axis. So, y_ are the correct values and y are the predicted values. tf.equal is used to find if they are equal or not."
      ]
    },
    {
      "metadata": {
        "id": "dqNSGNgW5byU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Evaluating the model\n",
        "prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lka4e4H6dcLL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4dfa3ee-abeb-41b4-898e-166c2ddfab99"
      },
      "cell_type": "code",
      "source": [
        "accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
        "print(sess.run(accuracy, feed_dict = {x:mnist.test.images, y_:mnist.test.labels}))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q6SW5k_AeWSH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Multi-convolutional layer\n",
        "Here we are using more layers, weights and biases to refine our model and increase accuracy. \n",
        "Our first convolutional layer has 32 features for each 5*5 patch. Its weight tensor will be of a shape of [5,5,1,32] . First two dimensions are the pathc size. the next is the input channel and the last number is the output channel.\n",
        "To apply the convolutional layer , x is reshaped to 4D tensor . 28*28 is the image width and height and the final dimension is the number of color channels.\n",
        "Then ReLU function is applied to bring the negative values to 0 and keep the positive vlaues as it is. Maxpool reduces the image size to 14 * 14.\n",
        "The second convolutinal layer has 64 featres for each 5*5 patch. Now that the image size has been reduced to 7*7 we add a fully-connected layer with 1024 neurons. "
      ]
    },
    {
      "metadata": {
        "id": "CKX8FLmQsMm0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Multilayer convolutional layer:\n",
        "\n",
        "#Functions to define weights and to be used in thecomputation ahead\n",
        "def weight_variable (shape):\n",
        "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def bias_variable (shape): \n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x,W):\n",
        "  return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding = 'SAME')\n",
        "\n",
        "def max_pool_2x2(x):\n",
        "  return tf.nn.max_pool(x, ksize=[1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
        "\n",
        "get_ipython().run_line_magic('pinfo', 'tf.nn.conv2d')\n",
        "\n",
        "#First Convolutional Layer\n",
        "W_conv1 = weight_variable([5,5,1,32])\n",
        "b_conv1 = bias_variable([32])\n",
        "\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
        "h_pool1 = max_pool_2x2(h_conv1)\n",
        "\n",
        "#Second Convolutional Layer\n",
        "W_conv2 = weight_variable([5, 5, 32, 64])\n",
        "b_conv2 = bias_variable([64])\n",
        "\n",
        "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
        "h_pool2 = max_pool_2x2(h_conv2)\n",
        "\n",
        "W_fc1 = weight_variable([7*7*64, 1024])\n",
        "b_fc1 = bias_variable([1024])\n",
        "\n",
        "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
        "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UCe55ec2B9oS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dropout and Readout Layer:\n",
        "To reduce overfitting we would drop some data randomly. Dropout layer thus helps in improving the accuracy of predictions. The next layer is used to read the data.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eL_NeUygeKHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Dropout Layer\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "\n",
        "#Readout Layer\n",
        "\n",
        "W_fc2 = weight_variable([1024, 10])\n",
        "b_fc2 = bias_variable([10])\n",
        "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ge_OyT7MDF0C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Final Accuracy:\n",
        "0.9919"
      ]
    },
    {
      "metadata": {
        "id": "nhXok74nDKHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3944
        },
        "outputId": "86afec60-a38f-4836-8e4f-b077e705f210"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  cross_entropy = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
        "  train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
        "  correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  for i in range(20000): #20000 are going to be the steps of the training set\n",
        "    batch = mnist.train.next_batch(50)\n",
        "    if i%100 == 0:\n",
        "      train_accuracy = accuracy.eval(feed_dict={\n",
        "          x:batch[0], y_:batch[1], keep_prob: 1.0})\n",
        "      print(\"step %d, training accuracy %g\"%(i,train_accuracy))\n",
        "      train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
        "      \n",
        "    print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
        "         x: mnist.test.images, y_:mnist.test.labels, keep_prob: 1.0\n",
        "     }))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, training accuracy 0.06\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "test accuracy 0.0865\n",
            "step 100, training accuracy 0.14\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "test accuracy 0.0678\n",
            "step 200, training accuracy 0.1\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n",
            "test accuracy 0.1372\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}